---
title: Użyj Apache Spark, aby odczytywać i zapisywać dane w Azure SQL Database
description: Dowiedz się, jak skonfigurować połączenie między klastrem usługi HDInsight Spark a Azure SQL Database do odczytu danych, zapisywania danych i przesyłania strumieniowego danych do bazy danych SQL
author: hrasheed-msft
ms.author: hrasheed
ms.reviewer: jasonh
ms.service: hdinsight
ms.topic: conceptual
ms.custom: hdinsightactive
ms.date: 03/05/2020
ms.openlocfilehash: 4e0c1626582297aa7d80cbbd4241b6f81e314f8f
ms.sourcegitcommit: 9cbd5b790299f080a64bab332bb031543c2de160
ms.translationtype: MT
ms.contentlocale: pl-PL
ms.lasthandoff: 03/08/2020
ms.locfileid: "78927458"
---
# <a name="use-hdinsight-spark-cluster-to-read-and-write-data-to-azure-sql-database"></a>Korzystanie z klastra Spark w usłudze HDInsight w celu odczytywania i zapisywania danych w Azure SQL Database

Dowiedz się, jak połączyć Klaster Apache Spark w usłudze Azure HDInsight przy użyciu Azure SQL Database, a następnie odczytywać, zapisywać i przesyłać strumieniowo dane do bazy danych SQL. Instrukcje w tym artykule używają [Jupyter Notebook](https://jupyter.org/) do uruchamiania fragmentów kodu Scala. Można jednak utworzyć autonomiczną aplikację w Scala lub Python i wykonać te same zadania.

## <a name="prerequisites"></a>Wymagania wstępne

* Azure HDInsight Spark klaster.  Postępuj zgodnie z instrukcjami w temacie [Tworzenie klastra Apache Spark w usłudze HDInsight](apache-spark-jupyter-spark-sql.md).

* Azure SQL Database. Postępuj zgodnie z instrukcjami w obszarze [tworzenie Azure SQL Database](../../sql-database/sql-database-get-started-portal.md). Upewnij się, że tworzysz bazę danych z przykładowym schematem **AdventureWorksLT** i danymi. Upewnij się również, że utworzono regułę zapory na poziomie serwera, aby umożliwić adres IP klienta dostępu do bazy danych SQL na serwerze. Instrukcje dotyczące dodawania reguły zapory są dostępne w tym samym artykule. Po utworzeniu Azure SQL Database upewnij się, że zachowasz następujące wartości. Są one potrzebne do łączenia się z bazą danych z klastra Spark.

    * Nazwa serwera hostującym Azure SQL Database.
    * Nazwa Azure SQL Database.
    * Azure SQL Database nazwę użytkownika/hasło administratora.

* SQL Server Management Studio (SSMS). Postępuj zgodnie z instrukcjami wyświetlanymi w [programie SSMS, aby nawiązywać połączenia i wykonywać zapytania dotyczące danych](../../sql-database/sql-database-connect-query-ssms.md).

## <a name="create-a-jupyter-notebook"></a>Tworzenie Jupyter Notebook

Zacznij od utworzenia [Jupyter Notebook](https://jupyter.org/) skojarzonej z klastrem Spark. Ten Notes służy do uruchamiania fragmentów kodu używanych w tym artykule.

1. W [Azure Portal](https://portal.azure.com/)Otwórz klaster.
1. Wybierz pozycję **Jupyter Notes** poniżej **pulpitów nawigacyjnych klastra** po prawej stronie.  Jeśli nie widzisz **pulpitów nawigacyjnych klastra**, wybierz pozycję **Przegląd** z menu po lewej stronie. Jeśli zostanie wyświetlony monit, wprowadź poświadczenia administratora klastra.

    ![Jupyter Notes na Apache Spark](./media/apache-spark-connect-to-sql-database/hdinsight-spark-cluster-dashboard-jupyter-notebook.png "Notes Jupyter na platformie Spark")

   > [!NOTE]  
   > Możesz również uzyskać dostęp do notesu Jupyter w klastrze Spark, otwierając następujący adres URL w przeglądarce. Zastąp ciąg **CLUSTERNAME** nazwą klastra:
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`

1. W notesie Jupyter w prawym górnym rogu kliknij pozycję **Nowy**, a następnie kliknij pozycję **Spark** , aby utworzyć Notes Scala. Notesy Jupyter w klastrze usługi HDInsight Spark udostępniają również jądro **PySpark** dla aplikacji python2 oraz jądra **PySpark3** dla aplikacji python3. W tym artykule utworzymy Notes Scala.

    ![Jądra dla notesu Jupyter na platformie Spark](./media/apache-spark-connect-to-sql-database/kernel-jupyter-notebook-on-spark.png "Jądra dla notesu Jupyter na platformie Spark")

    Aby uzyskać informacje na temat tych jąder, zobacz [Use Jupyter notebooks kernels with Apache Spark clusters in HDInsight](apache-spark-jupyter-notebook-kernels.md) (Używanie jąder notesu Jupyter z klastrami Apache Spark w usłudze HDInsight).

   > [!NOTE]  
   > W tym artykule używamy jądra Spark (Scala), ponieważ dane przesyłane strumieniowo z platformy Spark do usługi SQL Database są obecnie obsługiwane tylko w Scala i Java. Mimo że odczytywanie i zapisywanie w języku SQL można wykonać przy użyciu języka Python, aby zapewnić spójność w tym artykule, firma Microsoft używa Scala dla wszystkich trzech operacji.

1. Spowoduje to otwarcie nowego notesu o nazwie domyślnej, **bez tytułu**. Kliknij nazwę notesu i wprowadź wybraną nazwę.

    ![Podaj nazwę notesu](./media/apache-spark-connect-to-sql-database/hdinsight-spark-jupyter-notebook-name.png "Wprowadzanie nazwy notesu")

Teraz możesz rozpocząć tworzenie aplikacji.

## <a name="read-data-from-azure-sql-database"></a>Odczytaj dane z Azure SQL Database

W tej sekcji odczytujesz dane z tabeli (na przykład **tabeli SalesLT. Address**), która istnieje w bazie danych AdventureWorks.

1. W nowym notesie Jupyter w komórce kodu wklej poniższy fragment kodu i Zastąp wartości symboli zastępczych wartościami dla Azure SQL Database.

       // Declare the values for your Azure SQL database

       val jdbcUsername = "<SQL DB ADMIN USER>"
       val jdbcPassword = "<SQL DB ADMIN PWD>"
       val jdbcHostname = "<SQL SERVER NAME HOSTING SDL DB>" //typically, this is in the form or servername.database.windows.net
       val jdbcPort = 1433
       val jdbcDatabase ="<AZURE SQL DB NAME>"

    Naciśnij klawisze **SHIFT + ENTER**, aby uruchomić komórkę kodu.  

1. Użyj poniższego fragmentu, aby utworzyć adres URL JDBC, który można przekazać do interfejsów API Spark Dataframe. Kod tworzy obiekt `Properties` do przechowywania parametrów. Wklej ten fragment w komórce kodu i naciśnij klawisze **SHIFT + ENTER** , aby uruchomić.

       import java.util.Properties

       val jdbc_url = s"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=60;"
       val connectionProperties = new Properties()
       connectionProperties.put("user", s"${jdbcUsername}")
       connectionProperties.put("password", s"${jdbcPassword}")         

1. Skorzystaj z poniższego fragmentu, aby utworzyć ramkę danych zawierającą dane z tabeli w Azure SQL Database. W tym fragmencie kodu używamy tabeli `SalesLT.Address`, która jest dostępna w ramach bazy danych **AdventureWorksLT** . Wklej ten fragment w komórce kodu i naciśnij klawisze **SHIFT + ENTER** , aby uruchomić.

       val sqlTableDF = spark.read.jdbc(jdbc_url, "SalesLT.Address", connectionProperties)

1. Teraz można wykonywać operacje na ramce Dataframe, na przykład pobierając schemat danych:

       sqlTableDF.printSchema

    Zobaczysz dane wyjściowe podobne do następujących:

    ![dane wyjściowe schematu](./media/apache-spark-connect-to-sql-database/read-from-sql-schema-output.png "dane wyjściowe schematu")

1. Można również wykonywać operacje takie jak, pobierając 10 najważniejszych wierszy.

       sqlTableDF.show(10)

1. Lub Pobierz określone kolumny z zestawu danych.

       sqlTableDF.select("AddressLine1", "City").show(10)

## <a name="write-data-into-azure-sql-database"></a>Zapisywanie danych w Azure SQL Database

W tej sekcji używamy przykładowego pliku CSV dostępnego w klastrze, aby utworzyć tabelę w Azure SQL Database i wypełnić ją danymi. Przykładowy plik CSV (**HVAC. csv**) jest dostępny we wszystkich klastrach usługi HDInsight w `HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv`.

1. W nowym notesie Jupyter w komórce kodu wklej poniższy fragment kodu i Zastąp wartości symboli zastępczych wartościami dla Azure SQL Database.

       // Declare the values for your Azure SQL database

       val jdbcUsername = "<SQL DB ADMIN USER>"
       val jdbcPassword = "<SQL DB ADMIN PWD>"
       val jdbcHostname = "<SQL SERVER NAME HOSTING SDL DB>" //typically, this is in the form or servername.database.windows.net
       val jdbcPort = 1433
       val jdbcDatabase ="<AZURE SQL DB NAME>"

    Naciśnij klawisze **SHIFT + ENTER**, aby uruchomić komórkę kodu.  

1. Poniższy fragment kodu kompiluje adres URL JDBC, który można przekazać do interfejsów API Spark Dataframe. Kod tworzy obiekt `Properties` do przechowywania parametrów. Wklej ten fragment w komórce kodu i naciśnij klawisze **SHIFT + ENTER** , aby uruchomić.

       import java.util.Properties

       val jdbc_url = s"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=60;"
       val connectionProperties = new Properties()
       connectionProperties.put("user", s"${jdbcUsername}")
       connectionProperties.put("password", s"${jdbcPassword}")

1. Użyj poniższego fragmentu kodu, aby wyodrębnić schemat danych w pliku HVAC. csv i użyć schematu do załadowania danych z woluminu CSV w ramce Dataframe `readDf`. Wklej ten fragment w komórce kodu i naciśnij klawisze **SHIFT + ENTER** , aby uruchomić.

       val userSchema = spark.read.option("header", "true").csv("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv").schema
       val readDf = spark.read.format("csv").schema(userSchema).load("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv")

1. Użyj `readDf` Dataframe, aby utworzyć tabelę tymczasową `temphvactable`. Następnie użyj tabeli tymczasowej, aby utworzyć tabelę programu Hive, `hvactable_hive`.

       readDf.createOrReplaceTempView("temphvactable")
       spark.sql("create table hvactable_hive as select * from temphvactable")

1. Na koniec Użyj tabeli Hive, aby utworzyć tabelę w Azure SQL Database. Poniższy fragment kodu tworzy `hvactable` w Azure SQL Database.

       spark.table("hvactable_hive").write.jdbc(jdbc_url, "hvactable", connectionProperties)

1. Połącz się z Azure SQL Database przy użyciu programu SSMS i sprawdź, czy w tym miejscu znajduje się `dbo.hvactable`.

    a. Uruchom program SSMS i Połącz się z Azure SQL Database, podając szczegóły połączenia, jak pokazano na poniższym zrzucie ekranu.

    ![Nawiązywanie połączenia z usługą SQL Database za pomocą SSMS1](./media/apache-spark-connect-to-sql-database/connect-to-sql-db-ssms.png "Nawiązywanie połączenia z usługą SQL Database za pomocą SSMS1")

    b. W **Eksplorator obiektów**rozwiń Azure SQL Database i węzeł tabeli, aby wyświetlić utworzony obiekt **dbo. HVAC** .

    ![Nawiązywanie połączenia z usługą SQL Database za pomocą SSMS2](./media/apache-spark-connect-to-sql-database/connect-to-sql-db-ssms-locate-table.png "Nawiązywanie połączenia z usługą SQL Database za pomocą SSMS2")

1. Uruchom zapytanie w programie SSMS, aby zobaczyć kolumny w tabeli.

    ```sql
    SELECT * from hvactable
    ```

## <a name="stream-data-into-azure-sql-database"></a>Przesyłanie strumieniowe danych do Azure SQL Database

W tej sekcji przesyłamy strumieniowo dane do `hvactable`, które zostały już utworzone w Azure SQL Database w poprzedniej sekcji.

1. W pierwszym kroku upewnij się, że w `hvactable`nie ma żadnych rekordów. Za pomocą programu SSMS Uruchom następujące zapytanie w tabeli.

    ```sql
    TRUNCATE TABLE [dbo].[hvactable]
    ```

1. Tworzenie nowego notesu Jupyter w klastrze usługi HDInsight Spark. W komórce kodu wklej poniższy fragment kodu, a następnie naciśnij klawisze **SHIFT + ENTER**:

       import org.apache.spark.sql._
       import org.apache.spark.sql.types._
       import org.apache.spark.sql.functions._
       import org.apache.spark.sql.streaming._
       import java.sql.{Connection,DriverManager,ResultSet}

1. Przesyłamy strumieniowo dane z pliku **HVAC. csv** do `hvactable`. Plik HVAC. CSV jest dostępny w klastrze w `/HdiSamples/HdiSamples/SensorSampleData/HVAC/`. W poniższym fragmencie kodu najpierw uzyskasz schemat danych do przesyłania strumieniowego. Następnie tworzymy ramkę danych przesyłania strumieniowego przy użyciu tego schematu. Wklej ten fragment w komórce kodu i naciśnij klawisze **SHIFT + ENTER** , aby uruchomić.

       val userSchema = spark.read.option("header", "true").csv("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv").schema
       val readStreamDf = spark.readStream.schema(userSchema).csv("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/") 
       readStreamDf.printSchema

1. Dane wyjściowe pokazują Schemat pliku **HVAC. csv**. `hvactable` ma również ten sam schemat. W danych wyjściowych znajdują się kolumny w tabeli.

    ![tabela schematu Apache Spark usługi HDInsight](./media/apache-spark-connect-to-sql-database/hdinsight-schema-table.png "Schemat tabeli")

1. Na koniec użyj poniższego fragmentu kodu, aby odczytać dane z pliku HVAC. csv i przesłać je strumieniowo do `hvactable` w Azure SQL Database. Wklej ten fragment w komórce kodu, Zastąp wartości symboli zastępczych wartościami dla Azure SQL Database, a następnie naciśnij klawisze **SHIFT + ENTER** , aby uruchomić.

       val WriteToSQLQuery  = readStreamDf.writeStream.foreach(new ForeachWriter[Row] {
          var connection:java.sql.Connection = _
          var statement:java.sql.Statement = _
          
          val jdbcUsername = "<SQL DB ADMIN USER>"
          val jdbcPassword = "<SQL DB ADMIN PWD>"
          val jdbcHostname = "<SQL SERVER NAME HOSTING SDL DB>" //typically, this is in the form or servername.database.windows.net
          val jdbcPort = 1433
          val jdbcDatabase ="<AZURE SQL DB NAME>"
          val driver = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
          val jdbc_url = s"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;"
  
         def open(partitionId: Long, version: Long):Boolean = {
           Class.forName(driver)
           connection = DriverManager.getConnection(jdbc_url, jdbcUsername, jdbcPassword)
           statement = connection.createStatement
           true
         }
  
         def process(value: Row): Unit = {
           val Date  = value(0)
           val Time = value(1)
           val TargetTemp = value(2)
           val ActualTemp = value(3)
           val System = value(4)
           val SystemAge = value(5)
           val BuildingID = value(6)  
    
           val valueStr = "'" + Date + "'," + "'" + Time + "'," + "'" + TargetTemp + "'," + "'" + ActualTemp + "'," + "'" + System + "'," + "'" + SystemAge + "'," + "'" + BuildingID + "'"
           statement.execute("INSERT INTO " + "dbo.hvactable" + " VALUES (" + valueStr + ")")   
           }

         def close(errorOrNull: Throwable): Unit = {
            connection.close
          }
         })
        
         var streamingQuery = WriteToSQLQuery.start()

1. Sprawdź, czy dane są przesyłane strumieniowo do `hvactable`, uruchamiając następujące zapytanie w SQL Server Management Studio (SSMS). Za każdym razem, gdy uruchamiasz zapytanie, pokazuje liczbę wierszy w tabeli.

    ```sql
    SELECT COUNT(*) FROM hvactable
    ```

## <a name="next-steps"></a>Następne kroki

* [Analizowanie danych w Data Lake Storage przy użyciu klastra usługi HDInsight Spark](apache-spark-use-with-data-lake-store.md)
* [Przetwarzaj zdarzenia przesyłania strumieniowego ze strukturą przy użyciu centrum EventHub](apache-spark-eventhub-structured-streaming.md)
* [Używanie Apache Spark strukturalnych przesyłania strumieniowego z Apache Kafka w usłudze HDInsight](../hdinsight-apache-kafka-spark-structured-streaming.md)
